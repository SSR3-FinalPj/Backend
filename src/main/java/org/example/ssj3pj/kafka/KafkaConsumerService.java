package org.example.ssj3pj.kafka;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.example.ssj3pj.entity.EnvironmentMetadata;
import org.example.ssj3pj.services.EnvironmentDataService;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.JsonNode;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;

import org.example.ssj3pj.services.EnvironmentQueryService;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.stereotype.Service;
import java.util.List;
import java.util.ArrayList;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;

@Service
@Slf4j
@RequiredArgsConstructor
public class KafkaConsumerService {

    private final EnvironmentQueryService environmentDataService;
    private final ObjectMapper objectMapper = new ObjectMapper();

    @Value("${spring.topics.kafka.raw}")
    private String topicName;

    // ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì €ì¥ì†Œ
    private final List<EnvironmentMetadata> batchData = new ArrayList<>();
    private final AtomicInteger messageCount = new AtomicInteger(0);
    private static final int BATCH_SIZE = 120; // ì„œìš¸ ì‹¤ì‹œê°„ ë°ì´í„° ì´ëŸ‰
    private static final String COMPLETION_SIGNAL = "City data to ES Complete";
    private final DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");

    @KafkaListener(topics = "${spring.topics.kafka.raw}", groupId = "${spring.kafka.consumer.group-id}")
    public void consume(
            String jsonMessage,
            Acknowledgment acknowledgment,
            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
            @Header("kafka_receivedPartitionId") int partition,
            @Header("kafka_offset") long offset
    ) {
        try {
            log.info("\nğŸ”” Kafka Consumer ë©”ì‹œì§€ ìˆ˜ì‹ :");
            log.info("   Topic: {}, Partition: {}, Offset: {}", topic, partition, offset);
            log.info("   ë©”ì‹œì§€ ê¸¸ì´: {} characters", jsonMessage.length());
            log.info("   ë©”ì‹œì§€ ë‚´ìš©: '{}'", jsonMessage);
            log.info("   ì™„ë£Œ ì‹ í˜¸ì™€ ë¹„êµ: '{}' vs '{}'", jsonMessage.trim(), COMPLETION_SIGNAL);
            log.info("   ì™„ë£Œ ì‹ í˜¸ ë§¤ì¹­: {}", COMPLETION_SIGNAL.equals(jsonMessage.trim()));

            // ì™„ë£Œ ì‹ í˜¸ ì²´í¬ - JSON í˜•íƒœë¡œ ë³€ê²½
            if (isCompletionMessage(jsonMessage)) {
                log.info("âœ… ì™„ë£Œ ì‹ í˜¸ ê°ì§€! ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...");
                handleBatchCompletion(acknowledgment);
                return;
            }

            // ì¼ë°˜ ë°ì´í„° ì²˜ë¦¬
            EnvironmentMetadata environmentMetadata = parseEnvironmentMessage(jsonMessage);

            if (environmentMetadata != null) {
                synchronized (batchData) {
                    batchData.add(environmentMetadata);
                    int currentCount = messageCount.incrementAndGet();

                    log.info("   ë°°ì¹˜ ë°ì´í„° ì¶”ê°€: {}/{} (location: {}, es_doc_id: {}, source: {})",
                        currentCount, BATCH_SIZE,
                        environmentMetadata.getLocation(), environmentMetadata.getEsDocId(), environmentMetadata.getSource());

                    // ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë„ë‹¬ ì‹œ ê²½ê³  (ì™„ë£Œ ì‹ í˜¸ ëŒ€ê¸°)
                    if (currentCount >= BATCH_SIZE) {
                        log.warn("âš ï¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ({})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì™„ë£Œ ì‹ í˜¸ ëŒ€ê¸° ì¤‘...", BATCH_SIZE);
                    }
                }
            }

            acknowledgment.acknowledge();

        } catch (Exception e) {
            log.error("âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {}", e.getMessage(), e);
        }
    }

    /**
     * ì™„ë£Œ ì‹ í˜¸ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
     */
    private boolean isCompletionMessage(String jsonMessage) {
        try {
            JsonNode jsonNode = objectMapper.readTree(jsonMessage);

            // message í•„ë“œê°€ ìˆê³  "City data to ES Complete"ë¥¼ í¬í•¨í•˜ë©´ ì™„ë£Œ ì‹ í˜¸
            if (jsonNode.has("message")) {
                String message = jsonNode.get("message").asText();
                boolean isCompletion = COMPLETION_SIGNAL.equals(message);
                log.info("ğŸ” ì™„ë£Œ ì‹ í˜¸ ê²€ì‚¬: '{}' == '{}' => {}", message, COMPLETION_SIGNAL, isCompletion);
                return isCompletion;
            }

            return false;

        } catch (Exception e) {
            log.warn("ğŸ™ˆ ì™„ë£Œ ì‹ í˜¸ ê²€ì‚¬ ì¤‘ JSON íŒŒì‹± ì˜¤ë¥˜: {}", e.getMessage());
            return false;
        }
    }

    /**
     * JSON ë©”ì‹œì§€ë¥¼ EnvironmentMetadataë¡œ íŒŒì‹±
     */
    private EnvironmentMetadata parseEnvironmentMessage(String jsonMessage) {
        try {
            JsonNode jsonNode = objectMapper.readTree(jsonMessage);

            // ì™„ë£Œ ì‹ í˜¸ ì²´í¬
            if (jsonNode.has("message") && jsonNode.get("message").asText().contains("Complete")) {
                return null; // ì™„ë£Œ ì‹ í˜¸ëŠ” null ë°˜í™˜
            }

            // í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if (!jsonNode.has("es_doc_id") || !jsonNode.has("location") || !jsonNode.has("source")) {
                log.warn("âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {}", jsonMessage);
                return null;
            }

            EnvironmentMetadata metadata = new EnvironmentMetadata();

            // í•„ìˆ˜ í•„ë“œ ì„¤ì •
            metadata.setEsDocId(jsonNode.get("es_doc_id").asText());
            metadata.setLocation(jsonNode.get("location").asText());
            metadata.setSource(jsonNode.get("source").asText());

            // recorded_at ì„¤ì •
            if (jsonNode.has("recorded_at")) {
                metadata.setRecordedAt(jsonNode.get("recorded_at").asText());
            }

            // indexed_at ì„¤ì • - í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
            metadata.setIndexedAt(LocalDateTime.now());

            return metadata;

        } catch (Exception e) {
            log.error("âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {}, ë©”ì‹œì§€: {}", e.getMessage(), jsonMessage);
            return null;
        }
    }

    private void handleBatchCompletion(Acknowledgment acknowledgment) {
        synchronized (batchData) {
            int dataSize = batchData.size();
            log.info("\nğŸ¯ ë°°ì¹˜ ì™„ë£Œ ì‹ í˜¸ ìˆ˜ì‹ ! ì´ {}ê°œ ë°ì´í„° ì¼ê´„ ì €ì¥ ì‹œì‘...", dataSize);

            if (dataSize == 0) {
                log.warn("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.");
                acknowledgment.acknowledge();
                return;
            }

            try {
                // ë°°ì¹˜ë¡œ EnvironmentMetadataë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                environmentDataService.saveAll(new ArrayList<>(batchData));

                log.info("âœ… ë°°ì¹˜ ì €ì¥ ì™„ë£Œ! {}ê°œ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.", dataSize);
                log.info("   - ì˜ˆìƒ ë°°ì¹˜ í¬ê¸°: {}", BATCH_SIZE);
                log.info("   - ì‹¤ì œ ì €ì¥ í¬ê¸°: {}", dataSize);

                // ë°°ì¹˜ ë°ì´í„° ì´ˆê¸°í™”
                batchData.clear();
                messageCount.set(0);

                acknowledgment.acknowledge();
                log.info("âœ… ë°°ì¹˜ ì²˜ë¦¬ ë° ì»¤ë°‹ ì™„ë£Œ!\n");

            } catch (Exception e) {
                log.error("âŒ ë°°ì¹˜ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {}", e.getMessage(), e);
                // ì˜¤ë¥˜ ë°œìƒ ì‹œ ì»¤ë°‹í•˜ì§€ ì•ŠìŒ
            }
        }
    }
}
